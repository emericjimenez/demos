{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a2f100",
   "metadata": {},
   "source": [
    "# IA DETECT - Supervised Machine Learning\n",
    "## Model saved in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f6257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install joblib --user\n",
    "#!pip install spacy\n",
    "#!pip install nltk\n",
    "#!pip install xgboost\n",
    "#!pip install textblob\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('brown')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "from textblob import TextBlob \n",
    "import statistics\n",
    "from spacy.symbols import PUNCT\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518b389",
   "metadata": {},
   "source": [
    "## .csv already processed with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f18e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94890"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full = pd.read_csv(\"train_full.csv\", low_memory=False)\n",
    "train_full.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52809640",
   "metadata": {},
   "source": [
    "## subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a83b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50115\n",
      "44775\n",
      "87775\n"
     ]
    }
   ],
   "source": [
    "no = train_full[train_full[\"generated\"]==0]\n",
    "yes = train_full[train_full[\"generated\"]==1]\n",
    "print(no.shape[0])\n",
    "print(yes.shape[0])\n",
    "train_full_x = pd.concat([no[:43000], yes], axis=0)\n",
    "print(train_full_x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e570d5c",
   "metadata": {},
   "source": [
    "## Feature engineering for actual input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97deec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text1):\n",
    "    blob = TextBlob(text1)\n",
    "    blob.tags\n",
    "    blob.noun_phrases\n",
    "    list_Sentiment = []\n",
    "    for sentence in blob.sentences:\n",
    "        list_Sentiment.append(sentence.sentiment.polarity)        \n",
    "    st_dev = statistics.pstdev(list_Sentiment)\n",
    "    mean = statistics.mean(list_Sentiment)    \n",
    "    return mean, st_dev\n",
    "    \n",
    "def find_dif(text1, text2):            \n",
    "    blob1 = TextBlob(text1)\n",
    "    blob2 = TextBlob(text2)\n",
    "    tokens_text1 = blob1.words\n",
    "    tokens_text2 = blob2.words\n",
    "    diferency = set(tokens_text1) ^ set(tokens_text2)    \n",
    "    return diferency\n",
    "\n",
    "def process_blob(strText):    \n",
    "    strText_Blob = TextBlob(strText) \n",
    "    correct_Text = str(strText_Blob.correct())\n",
    "    diferency = find_dif(strText, correct_Text) \n",
    "    \n",
    "    _mean, _stddev = sentiment(correct_Text)\n",
    "    \n",
    "    if diferency:\n",
    "        return len(diferency)/2, _mean, _stddev        \n",
    "    else:\n",
    "        return 0, _mean, _stddev\n",
    "    \n",
    "\n",
    "def numPunct(text_objective):\n",
    "    doc = nlp(text_objective)\n",
    "    try:\n",
    "        num_words = len(doc)\n",
    "        stopwords = [token for token in doc if token.is_stop]\n",
    "        number_of_stopwords = len(stopwords)    \n",
    "        punctuation_tokens = [token for token in doc if token.pos == PUNCT]\n",
    "        number_of_punctuation_marks = len(punctuation_tokens)\n",
    "        \n",
    "        unique_words = set(token.text for token in doc)\n",
    "        count_unique_words = len(unique_words)\n",
    "        \n",
    "        words_doc = [token.text.lower() for token in doc if token.is_alpha]\n",
    "        frecuency_words = Counter(words_doc)\n",
    "        \n",
    "        most_word, frec_word = frecuency_words.most_common(1)[0]\n",
    "            \n",
    "        val_numwords = num_words/100\n",
    "       \n",
    "        porc_sw = number_of_stopwords/val_numwords\n",
    "        porc_punt = number_of_punctuation_marks/val_numwords\n",
    "        porc_unique_words = count_unique_words/val_numwords\n",
    "    \n",
    "        repet_it = sum(1 for token in doc if token.text.lower() == \"it\")\n",
    "        repet_is = sum(1 for token in doc if token.text.lower() == \"is\")\n",
    "      \n",
    "    except IndexError:      \n",
    "        print(\"An exception occurred\") \n",
    "        return 0, 0, 0, 0, 0, 0, \"0\", 0, 0, 0, 0\n",
    "    \n",
    "    print(f\"Number of words: {num_words}\")\n",
    "    print(f\"Number of Stop Words: {number_of_stopwords}\")\n",
    "    print(f\"Number of punctuation marks: {number_of_punctuation_marks}\")\n",
    "    print(f\"Number of unique words: {count_unique_words}\")\n",
    "    print(f\"Number of  it: {repet_it}\")\n",
    "    print(f\"Number of  is: {repet_is}\")\n",
    "    print(f\"Most repeated word: {most_word}\")\n",
    "    print(f\"Repeated word frequency: {frec_word}\")\n",
    "    \n",
    "    print(f\"Percentage of stopwords: {porc_sw}\")\n",
    "    print(f\"Percentage of punctuation marks: {porc_punt}\")\n",
    "    print(f\"Percentage of unique words: {porc_unique_words}\")\n",
    "    \n",
    "    return num_words, number_of_stopwords, number_of_punctuation_marks, count_unique_words, repet_it, repet_is, most_word, frec_word, porc_sw, porc_punt, porc_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90907c",
   "metadata": {},
   "source": [
    "## Train and save, load the model saved in the pickle file and use it for prediction!!!\n",
    "### The search for better hyperparameters and the importance of characteristics has already been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0151d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 87.91 %\n",
      "Curva ROC:  0.9227015554823788\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "numeric_cols = [\"bad_words\", \"w_mean\",  \"w_std\", \"num_words\", \"num_stop_words\", \"num_signs\", \"num_u_words\", \"it\", \"is\", \"frec_r_word\", \"per_sw\", \"per_sign\", \"perc_u_w\"]\n",
    "\n",
    "X = train_full_x[numeric_cols]\n",
    "y = train_full_x[\"generated\"]\n",
    "\n",
    "array_X = np.array(X)\n",
    "array_y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(array_X, array_y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_classifier = XGBClassifier(learning_rate=0.1, max_depth=6, n_estimators=300)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "y_prob = xgb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "joblib.dump(xgb_classifier, 'xg_model.pkl')\n",
    "xg_model = joblib.load('xg_model.pkl')\n",
    "score = xg_model.score(X_train, y_train)\n",
    "\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "print(\"Curva ROC: \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7de31",
   "metadata": {},
   "source": [
    "## We use the complete csv that only has the generated columns and text\n",
    "### The test_texts.csv file contains various data that we did not use for training, so we use it for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820972c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94890"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = pd.read_csv(\"test_texts.csv\", low_memory=False)\n",
    "test_text.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299de12",
   "metadata": {},
   "source": [
    "## Test ---> We try with the texts of the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33b7d2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 256\n",
      "Number of Stop Words: 113\n",
      "Number of punctuation marks: 29\n",
      "Number of unique words: 133\n",
      "Number of  it: 4\n",
      "Number of  is: 5\n",
      "Most repeated word: the\n",
      "Repeated word frequency: 12\n",
      "Percentage of stopwords: 44.140625\n",
      "Percentage of punctuation marks: 11.328125\n",
      "Percentage of unique words: 51.953125\n"
     ]
    }
   ],
   "source": [
    "row_index =89562\n",
    "t1 = test_text[\"text\"][row_index]\n",
    "bad_words, w_mean, w_std = process_blob(t1)\n",
    "num_words, num_stop_words, num_signs, num_u_words, it, is_, repeat_word, frec_r_word, per_sw, per_sign, perc_u_w = numPunct(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529a87b",
   "metadata": {},
   "source": [
    "## Shows the probability of being AI-generated text.\n",
    "## The closer it is to 1 the more likely it is to be AI-generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f795ece8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability IA generated:  0.9478052\n",
      "Value of csv:  1\n"
     ]
    }
   ],
   "source": [
    "list_values = [bad_words, w_mean, w_std, num_words, num_stop_words, num_signs, num_u_words, it, is_, frec_r_word, per_sw, per_sign, perc_u_w]\n",
    "array_values = np.array(list_values)\n",
    "y_prob = xg_model.predict_proba([array_values])[:, 1]\n",
    "print(\"Probability IA generated: \", y_prob[0])\n",
    "print(\"Value of csv: \", test_text[\"generated\"][row_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f294f51",
   "metadata": {},
   "source": [
    "## We show the text used and the value of \"generated\" in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7333ac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computer simulation is a technique for using a computer to model the behavior of a system or process. It involves creating a mathematical model of the system or process, and then using a computer to run simulations based on that model. The goal of a simulation is to mimic the behavior of the system or process as closely as possible, in order to gain insights, make predictions, or test hypotheses.\\n\\n\\n\\nSimulations can be used in a wide range of fields, including engineering, science, business, and social science. For example, simulations can be used to study the behavior of aircraft, predict the spread of diseases, or analyze economic trends. Simulations are often used in situations where it is not practical or possible to conduct experiments in the real world, or when it is more cost-effective to perform simulations rather than build and test physical prototypes.\\n\\n\\n\\nThere are many different types of computer simulations, ranging from simple models with a few variables to complex simulations that involve multiple interacting variables and processes. Some simulations are designed to be interactive, allowing users to alter the model and see how it affects the results. Others are run automatically, with the results being analyzed and presented in the form of graphs, charts, or tables.\\n\\n\\n\\nOverall, computer simulation is a powerful tool that can help us better understand complex systems and make informed decisions.']\n",
      "--------------------->Value of generated in csv:  1\n"
     ]
    }
   ],
   "source": [
    "print(test_text[\"text\"][row_index])\n",
    "print(\"--------------------->Value of generated in csv: \", test_text[\"generated\"][row_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de112ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
